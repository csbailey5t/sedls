{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.0 64-bit ('sedls-EhSD_cOU-py3.8': venv)",
   "display_name": "Python 3.8.0 64-bit ('sedls-EhSD_cOU-py3.8': venv)",
   "metadata": {
    "interpreter": {
     "hash": "74e3fe4f46643bf5345544bab31527dfc819d8a420bac11c9d0fbbf74c17ef0e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exploratory Text Analysis with Python\n",
    "\n",
    "Workshop for the Southeast Data Librarian Symposium 2020\n",
    "\n",
    "Scott Bailey <br/>\n",
    "Digital Research and Scholarship Librarian <br/>\n",
    "Copyright and Digital Scholarship Center <br/>\n",
    "NC State University Libraries\n",
    "\n",
    "## Outline\n",
    "1. Intro and overview of NLP libraries\n",
    "2. Document-level analysis <br/>\n",
    "    a. Tokenization <br/>\n",
    "    b. Cleaning text data <br />\n",
    "    c. Part-of-speech tagging <br/>\n",
    "    d. Named entity recognition <br/>\n",
    "    e. Similarity vectors <br/>\n",
    "    f. Rule-based matching <br />\n",
    "3. Scaling up to corpus-level analysis\n",
    "4. Further resources for spaCy\n",
    "\n",
    "## What do we mean by \"exploratory text analysis?\"\n",
    "- How clean are the data?\n",
    "- What methods do the data support?\n",
    "- Project scoping \n",
    "- Research question refinement\n",
    "- Iterative research \n",
    "\n",
    "## A quick(!) overview of NLP-related libraries in Python\n",
    "- [nltk](https://www.nltk.org/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/)\n",
    "- [stanza/corenlp](https://stanfordnlp.github.io/stanza/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [huggingface transformers - pytorch and tensorflow](https://github.com/huggingface/transformers)\n",
    "\n",
    "### Why spaCy?\n",
    "\n",
    "An opinionated, performant NLP that does a lot of the work for you while revealing where you might need to do more custom refinement or model building. \n",
    "\n",
    "## Questions during the workshop\n",
    "\n",
    "During the workshop, please do ask questions by way of the Zoom chat. I'll be keeping an eye on that, and will answer questions as we go. I'll also give some time during and after the workshop when folks can unmute and ask questions. \n",
    "\n",
    "## Jupyter Notebooks, Google Colab, and Binder\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if working in Google Colab or Binder\n",
    "# If working locally, add spaCy to your environment in the preferred way\n",
    "# and in a shell with that environment, run the model download\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://se-datalibrarian.github.io/2020/about/\n",
    "# I've added the final, untrue sentence, though, to make sure we have entities for when we hit named entity recognition.\n",
    "sample_text = \"\"\"The Southeast Data Librarian Symposium is intended to provide an opportunity for librarians and other research data specialists to explore developments in the field of data librarianship, including the management and sharing of research data.\n",
    "\n",
    "In addition to learning about new work in the field, attendees will have the opportunity to network and build partnerships with regional colleagues. It is open to all who wish to attend, including students, data managers, and data scientists.\n",
    "\n",
    "The Symposium has previously taken place in Athens, Georgia, and has been sponsored by Google for $10 million.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)"
   ]
  },
  {
   "source": [
    "## Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in doc[:20]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "source": [
    "## Cleaning text data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One of the common things we do in text analysis is to remove punctuation\n",
    "no_punct = [token for token in doc if token.is_punct == False]\n",
    "for token in no_punct[:50]:\n",
    "  print(token.text, token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This has worked, but left in new line characters and spaces\n",
    "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
    "for token in no_punct_or_space[:30]:\n",
    "  print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we also want to remove numbers, and lowercase everything\n",
    "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
    "lower_alpha[:30]"
   ]
  },
  {
   "source": [
    "One other common bit of preprocessing is to remove stopwords, that is, the common words in a language that don't convey the information that we are looking for in our analysis. For example, if we looked for the most common words in a text, we would want to remove stopwords so that we don't only get words such as 'a,' 'the,' and 'and.'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
    "clean[:30]"
   ]
  },
  {
   "source": [
    "For this piece, we've used spaCy's built in stopword list, which is used to create the property `is_stop` for each token. There's a good chance you would want to create custom stopwords lists though, especially if you're working with historical text or really domain-specific text. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just pick a couple of words we know are in the example\n",
    "custom_stopwords = [\"developments\", \"management\"]\n",
    "\n",
    "custom_clean = [token.lower_ for token in doc if token.lower_ not in custom_stopwords]\n",
    "custom_clean"
   ]
  },
  {
   "source": [
    "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on our analysis, we may or may not want to do this much cleaning. But, it is good to understand how much we can do just with spaCy. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Since we can break apart the document and filter it now, it's a good time to start counting things"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of tokens in document: \", len(doc))\n",
    "print(\"Number of tokens in cleaned document: \", len(clean))\n",
    "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sentences\n",
    "len(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all lower-cased tokens\n",
    "full_counter = Counter([token.lower_ for token in doc])\n",
    "full_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count cleaned tokens\n",
    "cleaned_counter = Counter(clean)\n",
    "cleaned_counter.most_common(20)"
   ]
  },
  {
   "source": [
    "**Question:** Why do we have to use a list comprehension for the non-clean doc while we can just pass a variable directly for the cleaned set of tokens?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Activity\n",
    "\n",
    "In the cell below, write code to find the five most common noun chunks in the original doc. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "source": [
    "## Part-of-speech tagging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coarse grained UPOS: https://universaldependencies.org/docs/u/pos/\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-grained POS, Penn Treebank: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "source": [
    "# Not sure what those tags are? Try spaCy's explain function\n",
    "spacy.explain(\"DT\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tokens by part of speech\n",
    "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect plural nouns\n",
    "nouns_pl = [token for token in doc if token.tag_ == \"NNS\" or token.tag_ == \"NNPS\"]\n",
    "nouns_pl"
   ]
  },
  {
   "source": [
    "### Dependency tree visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentence = list(doc.sents)[0]\n",
    "single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spaCy determines the dependency tree for it's doc. Like POS, we can see the dependency tags of each token. \n",
    "for token in single_sentence:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(single_sentence, style=\"dep\")"
   ]
  },
  {
   "source": [
    "## Named entity recognition\n",
    "\n",
    "[List of entity types in spaCy](https://spacy.io/api/annotation#named-entities)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)    "
   ]
  },
  {
   "source": [
    "### Activity\n",
    "\n",
    "Add or modify a sentence in the original `sample_text` so that spaCy will detect a PERSON. Then, in the cell below, write code to return a list of all entities that are either PERSON or GPE.\n",
    "\n",
    "**hint**: make sure to reprocess the `sample_text` with the `nlp` model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "source": [
    "### Visualizing named entities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentence = list(doc.sents)[-1]\n",
    "displacy.render(single_sentence, style=\"ent\")"
   ]
  },
  {
   "source": [
    "## Word, sentence, and document vectors\n",
    "\n",
    "SpaCy's medium (`md`) and large (`lg`) models include GloVe word vectors trained on the [Common Crawl](https://commoncrawl.org/). \n",
    "\n",
    "You could train your own vectors with `gensim` and `word2vec`, use a large language model, or many other libraries and algorithms. But, if you're text is fairly recent and especially from the web, the common crawl vectors might be enough, especially for exploratory work. \n",
    "\n",
    "`Token`s have vectors. `Doc`s and `Span`s have vectors that are the average of their token vectors. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token vectors\n",
    "for token in doc[:5]:\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc vector\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence/span vector\n",
    "list(doc.sents)[0].vector"
   ]
  },
  {
   "source": [
    "This is fine, but for exploratory work, we might just be interested in some similarity measures between tokens, sentences, or documents. SpaCy uses the common cosine similarity measure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token1 in doc[:10]:\n",
    "    for token2 in doc[:10]:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "source": [
    "**Question**: Looking at the results, can you explain the scale of the similarity score?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent1 in doc.sents:\n",
    "    for sent2 in doc.sents:\n",
    "        print(sent1.text, sent2.text, \"\\n\", sent1.similarity(sent2))\n",
    "        print(\"----------------------------------------------\")"
   ]
  },
  {
   "source": [
    "## Rule based matcher\n",
    "\n",
    "Rule-based matching is an incredibly powerful complement to the statistic models of spaCy. It's also a bit complex though, and it's worth looking at the docs [here](https://spacy.io/usage/rule-based-matching)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "source": [
    "[Available token attributes for the `Matcher` pattern](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'LOWER': 'symposium'},\n",
    "           {'DEP': 'aux'}]\n",
    "matcher.add(\"sympo+aux\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "source": [
    "One of the easiest ways to build up these `Matcher` patterns is to use their online [Rule-based Matcher Explorer](https://explosion.ai/demos/matcher). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Working with multiple documents (a corpus)\n",
    "\n",
    "For a small corpus, you can build a list or dictionary of processed spaCy docs. Once you have that list or dictionary, approach it in terms of using the type of code we've written above, but applied over the larger data structure. \n",
    "\n",
    "For larger corpora, though, you might need to think about streaming data or distributed processing. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/csbailey5t/sedls/blob/master/aspca-texts.zip\n",
    "!unzip aspca-texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = glob.glob(\"texts/*.txt\")\n",
    "len(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for fn in fns:\n",
    "    with open(fn, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time corpus = [nlp(text) for text in texts[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all geo-political entities from whole corpus\n",
    "gpes = [(ent.text, ent.label_) for ent in doc.ents for doc in corpus if ent.label_ == \"GPE\"]\n",
    "len(gpes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of unique GPEs\n",
    "set(gpes)"
   ]
  },
  {
   "source": [
    "### Activity\n",
    "\n",
    "Choose a method from the single document analysis portion of the workshop, and apply it to this small corpus. For example, you could find the most common words, create a cleaned corpus, or aggregate parts of speech. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "source": [
    "spaCy also provides a `pipe` method on the language model that should batch your document processing. This can be useful for larger collections of texts. We'll only see a small advantage in our small corpus, but it gets more significant as you batch in larger sizes with more processes. \n",
    "\n",
    "https://spacy.io/api/language#pipe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time docs = [nlp(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time docs = list(nlp.pipe(texts, batch_size=10, n_process=1))"
   ]
  },
  {
   "source": [
    "## Resources for spaCy\n",
    "\n",
    "- [spaCy 101](https://spacy.io/usage/spacy-101) - spaCy's own intro documentation\n",
    "- [Advanced NLP with spaCy](https://course.spacy.io/) - spaCy's own interactive learning course; you don't need to be \"ready\" for \"advanced\" work to benefit from going through this course\n",
    "- [textacy](https://github.com/chartbeat-labs/textacy) - a Python library built on top of spaCy and scikit-learn to faciliate working with a corpus and providing extra functionality\n",
    "- [spaCy universe](https://spacy.io/universe) - extensive collection of packages built on top of or with spaCy for various NLP and text analysis tasks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Activity?\n",
    "\n",
    "I'm happy to stay on for a while and answer questions or help if anyone would like to work with one of their own texts in spaCy to try out some of these techniques/approaches."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}