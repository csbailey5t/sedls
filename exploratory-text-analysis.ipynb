{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.0 64-bit",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "74e3fe4f46643bf5345544bab31527dfc819d8a420bac11c9d0fbbf74c17ef0e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exploratory Text Analysis with Python\n",
    "\n",
    "Workshop for the Southeast Data Librarian Symposium 2020\n",
    "\n",
    "Scott Bailey <br/>\n",
    "Digital Research and Scholarship Librarian <br/>\n",
    "Copyright and Digital Scholarship Center <br/>\n",
    "NC State University Libraries\n",
    "\n",
    "## What do we mean by \"exploratory text analysis?\"\n",
    "\n",
    "## A quick (!) overview of NLP libraries in Python\n",
    "### Why spaCy?\n",
    "\n",
    "## Jupyter Notebooks, Google Colab, and Binder\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if working in Google Colab or Binder\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://se-datalibrarian.github.io/2020/about/\n",
    "# I've added the final, untrue sentence, though, to make sure we have entities for when we hit named entity recognition.\n",
    "sample_text = \"\"\"The Southeast Data Librarian Symposium is intended to provide an opportunity for librarians and other research data specialists to explore developments in the field of data librarianship, including the management and sharing of research data.\n",
    "\n",
    "In addition to learning about new work in the field, attendees will have the opportunity to network and build partnerships with regional colleagues. It is open to all who wish to attend, including students, data managers, and data scientists.\n",
    "\n",
    "The Symposium has previously taken place in Athens, Georgia, and has been sponsored by Google for $10 million.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)"
   ]
  },
  {
   "source": [
    "## Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in doc[:20]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "source": [
    "## Part-of-speech tagging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coarse grained UPOS: https://universaldependencies.org/docs/u/pos/\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-grained POS, Penn Treebank: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tokens by part of speech\n",
    "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect plural nouns\n",
    "nouns_pl = [token for token in doc if token.tag_ == \"NNS\" or token.tag_ == \"NNPS\"]\n",
    "nouns_pl"
   ]
  },
  {
   "source": [
    "### Visualization dependency tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentence = list(doc.sents)[0]\n",
    "single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(single_sentence, style=\"dep\")"
   ]
  },
  {
   "source": [
    "## Named entity recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)    "
   ]
  },
  {
   "source": [
    "### Visualizing named entities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentence = list(doc.sents)[-1]\n",
    "displacy.render(single_sentence, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "source": [
    "## Word, sentence, and document vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity"
   ]
  },
  {
   "source": [
    "## Cleaning text data\n",
    "\n",
    "- stopwords\n",
    "- lemmas\n",
    "- punctuation\n",
    "- alphanum\n",
    "- lowercase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Working with multiple documents (a corpus)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For a small corpus, you can build a list of processed spaCy docs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/csbailey5t/sedls/blob/master/aspca-texts.zip\n",
    "!unzip aspca-texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = glob.glob(\"texts/*.txt\")\n",
    "len(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for fn in fns:\n",
    "    with open(fn, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time corpus = [nlp(text) for text in texts[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all geo-political entities from whole corpus\n",
    "gpes = [(ent.text, ent.label_) for ent in doc.ents for doc in corpus if ent.label_ == \"GPE\"]\n",
    "len(gpes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of unique GPEs\n",
    "set(gpes)"
   ]
  },
  {
   "source": [
    "spaCy also provides a `pipe` method on the language model that should batch your document processing. This can be useful for larger collections of texts. We'll only see a small advantage in our small corpus, but it gets more significant as you batch in larger sizes with more processes. \n",
    "\n",
    "https://spacy.io/api/language#pipe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time docs = [nlp(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time docs = list(nlp.pipe(texts, batch_size=10, n_process=1))"
   ]
  },
  {
   "source": [
    "## Resources for spaCy\n",
    "\n",
    "- [spaCy 101](https://spacy.io/usage/spacy-101)\n",
    "- [Advanced NLP with spaCy](https://course.spacy.io/)\n",
    "- [textacy](https://github.com/chartbeat-labs/textacy) - a Python library built on top of spaCy and scikit-learn to faciliate working with a corpus and providing extra functionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}